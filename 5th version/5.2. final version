import json
import re
import device
import nltk
import torch
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer
from transformers import BertForSequenceClassification
from torch.utils.data import DataLoader, Dataset
from torch.optim import AdamW
from sklearn.metrics import (accuracy_score, f1_score, confusion_matrix, balanced_accuracy_score, log_loss)
from imblearn.over_sampling import SMOTE
import numpy as np
import matplotlib.pyplot as plt
from torch.cuda.amp import GradScaler, autocast
from datetime import datetime
from sklearn.utils import class_weight
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import KFold
from nltk.corpus import stopwords
from torch.nn.functional import softmax
import torch.nn as nn


class SentimentDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')
stop_words = set(stopwords.words('russian'))  # russian stop words


def clean_text(text):  # removing the links, transforming to lower register, removing the symbols
    text = re.sub(r'http\S+', ' ', text)
    text = re.sub(r'[^a-zA-Zа-яА-Я0-9\s]', ' ', text)
    # text = re.sub(r'\s+', ' ', text).strip() - if we wanna remove whole symbols
    return text.lower()


def remove_stop_words(text_list):  # removing some russian stop words
    cleaned_text_list = []
    for text in text_list:
        words = text.split()
        filtered_words = [word for word in words if word.lower() not in stop_words]
        cleaned_text_list.append(' '.join(filtered_words))
    return cleaned_text_list


def add_context_embedding(texts, contexts):
    new_texts = []
    for i, text in enumerate(texts):
        context = contexts.get(i)  # return None if the key is not found
        if context is not None:
            new_text = context['before'] + text + context['after']
            new_texts.append(new_text)
        else:
            new_texts.append(text)  # handle the case where context is not found
    return new_texts


def main():
    global current_val_loss, additional_context_val, additional_context_train
    with open('train.json', 'r', encoding='utf-8') as file:
        data = json.load(file)

    texts = [item['text'] for item in data]
    labels = [item['sentiment'] for item in data]
    label_dict = {'positive': 0, 'neutral': 1, 'negative': 2}
    labels = [label_dict[label] for label in labels]

    kf = KFold(n_splits=4)  # 25/75 - change.. 25/75......
    fold_results = []
    for fold, (train_index, val_index) in enumerate(kf.split(data)):
        train_data = [data[i] for i in train_index]
        val_data = [data[i] for i in val_index]
        model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=3)
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model.to(device)
        train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.25,
                                                                            random_state=42)

        # Calculate class weights
        unique_classes = np.unique(train_labels)
        weights = class_weight.compute_class_weight(class_weight='balanced', classes=unique_classes, y=train_labels)
        class_weights = dict(zip(unique_classes, weights))
        class_weights_list = [class_weights[key] for key in sorted(class_weights.keys())]
        weights_tensor = torch.tensor(class_weights_list, dtype=torch.float)

        # apply resampling techniques to the training data
        vectorizer = TfidfVectorizer()
        X_train_vect = vectorizer.fit_transform(train_texts)
        sm = SMOTE(random_state=42)
        X_res, y_res = sm.fit_resample(X_train_vect, train_labels)

        # transforming my text
        cleaned_train_texts = [clean_text(text) for text in train_texts]
        cleaned_val_texts = [clean_text(text) for text in val_texts]

        removed_stop_words_train_texts = remove_stop_words(cleaned_train_texts)
        removed_stop_words_val_texts = remove_stop_words(cleaned_val_texts)

        for _ in train_texts:
            additional_context_train = {'before': 'Previous sentence.', 'after': 'Next sentence.'}
        for _ in val_texts:
            additional_context_val = {'before': 'Previous sentence.', 'after': 'Next sentence.'}
        train_texts_with_context = add_context_embedding(cleaned_train_texts, additional_context_train)
        val_texts_with_context = add_context_embedding(cleaned_val_texts, additional_context_val)

        tokenizer = BertTokenizer.from_pretrained(
            'bert-base-multilingual-cased')  # DistilBertTokenizer instead of BertTokenizer, because it's smaller and faster working
        train_encodings = tokenizer.batch_encode_plus(train_texts_with_context, max_length=512, padding='longest',
                                                      truncation=True)  # 512->256 for time optimizing
        val_encodings = tokenizer.batch_encode_plus(val_texts_with_context, max_length=512, padding='longest',
                                                    truncation=True)  # 512->256 for time optimizing

        loss_function = nn.CrossEntropyLoss(weight=weights_tensor)

        train_dataset = SentimentDataset(train_encodings, train_labels)
        val_dataset = SentimentDataset(val_encodings, val_labels)

        # print("BERT-model\n")
        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4,
                                  pin_memory=True)  # batch size changed 8->16 for generalization of the model
        val_loader = DataLoader(val_dataset, batch_size=16)
        optimizer = AdamW(model.parameters(), lr=1e-5,
                          eps=1e-6)  # l2-regularization: changed for speed up the training 1. learning rate, 2. exclude division by zero, 3. regularization via penalty
        scaler = GradScaler()  # for mixed training

        print("training\n")
        new_learning_rate = None
        best_val_loss = 10.0  # it'll stop, when the validation loss will start increasing
        early_stopping_counter = 0
        num_epochs = 5
        best_accuracy = 0.0
        log_loss_value = 0.0
        bal_acc = 0.0
        f1 = 0.0
        accuracy = 0.0
        total_loss = 0.0
        current_loss = 0.0
        for epoch in range(num_epochs):
            model.train()
            for batch in train_loader:
                inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}
                labels = batch['labels'].to(device)
                outputs = model(**inputs, labels=labels)
                loss = loss_function(outputs.squeeze(), labels)
                #loss = outputs.loss
                loss.backward()
                optimizer.step()
                optimizer.zero_grad()
                total_loss += loss.item()
                current_loss = loss.item()
            average_loss = total_loss / len(train_loader)

            print("loss:", loss)
            print("total_loss:", total_loss)
            print("current_loss:", current_loss)
            print("average_loss:", average_loss)

            print("mixed precision training\n")  # it allows us to take 16-bit floating point, 32-bit floating point & speed us up
            model.eval()
            with autocast():
                outputs = model(**inputs, labels=labels)
                loss = loss_function(outputs.squeeze(), labels)
                #loss = outputs.loss
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()

            print("validation loop\n")
            model.eval()
            predictions = []
            true_labels = []
            with torch.no_grad():
                for batch in val_loader:
                    inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}
                    labels = batch['labels'].to(device)
                    outputs = model(**inputs)
                    loss = loss_function(outputs.squeeze(), labels)
                    #loss = outputs.loss
                    preds = torch.argmax(outputs.logits, dim=1)
                    predictions.extend(preds.cpu().numpy())
                    true_labels.extend(labels.cpu().numpy())
                print("loss", loss)

            print("confusion_matrix\n")
            cm = confusion_matrix(true_labels, predictions,
                                  labels=[0, 1, 2])  # labels should be the sorted unique classes
            print(f"cm output:\n {cm}")
            plt.figure(figsize=(10, 7))
            plt.title('Normalized Confusion Matrix')
            plt.xlabel('Predicted')
            plt.ylabel('True')
            plt.show()
            plt.savefig('plot.png')

            print("normalized confusion matrix\n")
            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            print(f"cm output:\n {cm_normalized}")
            plt.figure(figsize=(10, 7))
            plt.title('Normalized Confusion Matrix')
            plt.xlabel('Predicted')
            plt.ylabel('True')
            plt.show()
            plt.savefig('plot.png')

            # evaluation of best model
            print("evaluation\n")
            model.eval()
            predictions = []
            true_labels = []
            with torch.no_grad():
                current_val_loss = 0.0
                for batch in val_loader:
                    inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}
                    labels = batch['labels'].to(device)
                    outputs = model(**inputs)
                    preds = torch.argmax(outputs.logits, dim=1)
                    predictions.extend(preds.cpu().numpy())
                    true_labels.extend(labels.cpu().numpy())
                if len(val_loader) != 0:
                    current_val_loss /= len(val_loader)
                    print(f"current_val_loss is {current_val_loss} of {epoch} epoch")
                else:
                    print(f"len(val_loader) is to be 0: {len(val_loader)}")

            # early stopping / decreasing the learning rate
            if current_val_loss < best_val_loss:
                best_val_loss = current_val_loss
                # torch.save(model.state_dict(), 'best_model_state.bin') # temporary save
            else:  # occur, if we stopped development
                early_stopping_counter += 1
                print(f" decrease the learning rate. Step number {early_stopping_counter}\n")
                print(
                    f"Epoch(epoch as output): {epoch} of {num_epochs} epoches ; Training Loss(average training loss as output): "
                    f"{loss:.3f} - loss, Validation Loss(current val loss as output): {current_val_loss:.3f}")
                if early_stopping_counter > 3:
                    for param_group in optimizer.param_groups:
                        new_learning_rate = max(param_group['lr'] - 1e-5,
                                                1e-6)  # delta will improve the model to the minimal
                        param_group['lr'] = new_learning_rate
                    early_stopping_counter = 0  # reset counter
                    print(f"No improvement for {epoch} consecutive epochs => decrease learning rate")
                    if new_learning_rate == 1e-6:
                        break

            if epoch == num_epochs:
                folder_name = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')  # generating new folder
                model_path = f'saved_models/{folder_name}/model'
                tokenizer_path = f'saved_models/{folder_name}/tokenizer'
                print("model&tokenizer saved\n")
                model.save_pretrained(f'C:/Users/user/pythonProject/{model_path}')  # save the model
                tokenizer.save_pretrained(f'C:/Users/user/pythonProject/{tokenizer_path}')  # save the tokenizer


            print("current_metrics\n") # they're written completely without macro/micro/weighted because of error during the running
            accuracy = accuracy_score(true_labels, predictions)
            f1 = f1_score(true_labels, predictions)
            bal_acc = balanced_accuracy_score(true_labels, predictions)
            predicted_probabilities = model.predict_proba(data)
            log_loss_value = log_loss(true_labels, predicted_probabilities, labels=[0, 1, 2])

            print(f"Accuracy: {accuracy}")
            print(f"F1 Score: {f1}")
            print(f"balanced_accuracy_score: {bal_acc}")
            print(f"log_loss : {log_loss_value}")


        #showing final results
        print(f"the final results of {epoch} epoch:")
        print(f"Accuracy: {accuracy}")
        print(f"F1 Score: {f1}")
        if log_loss_value != None:
            print(f"log_loss : {log_loss_value}")
        print(f"balanced_accuracy_score: {bal_acc}")

        # saving the results
        fold_results.append(accuracy)
        fold_results.append(f1)
        if log_loss_value != None:
            fold_results.append(log_loss_value)
        fold_results.append(bal_acc)


if __name__ == '__main__':
    main()
