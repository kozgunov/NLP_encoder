import json
import re
import device
import nltk
import torch
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer
from transformers import BertForSequenceClassification
from torch.utils.data import DataLoader, Dataset
from torch.optim import AdamW
from sklearn.metrics import (accuracy_score, f1_score, confusion_matrix, balanced_accuracy_score, log_loss)
from imblearn.over_sampling import SMOTE
import numpy as np
import matplotlib.pyplot as plt
from torch.cuda.amp import GradScaler, autocast
from datetime import datetime
from sklearn.utils import class_weight
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import KFold
from nltk.corpus import stopwords
from torch.nn.functional import softmax
#from transformers import AdamW

class SentimentDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    '''
    def __getitem__(self, idx):
        if idx >= len(self.encodings['input_ids']):  # assuming 'input_ids' is a key in encodings
            raise IndexError(
                f"Index {idx} is out of bounds for this dataset with length {len(self.encodings['input_ids'])}")
        input_ids = torch.tensor(self.encodings['input_ids'][idx]).unsqueeze(0)  # Add a batch dimension
        attention_mask = torch.tensor(self.encodings['attention_mask'][idx]).unsqueeze(0)  # Add a batch dimension
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        return item
    '''
    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

print("purification is started")
nltk.download('stopwords')
stop_words = set(stopwords.words('russian'))  # russian stop words

def clean_text(text): # removing the links, transforming to lower register, removing the symbols
    text = re.sub(r'http\S+', ' ', text)
    text = re.sub(r'[^a-zA-Zа-яА-Я0-9\s]', ' ', text)
    #text = re.sub(r'\s+', ' ', text).strip() - only if we wanna remove whole symbols
    return text.lower()

def remove_stop_words(text_list):# removing some russian stop words
    cleaned_text_list = []
    for text in text_list:
        words = text.split()
        filtered_words = [word for word in words if word.lower() not in stop_words]
        cleaned_text_list.append(' '.join(filtered_words))
    return cleaned_text_list


# print("context embedding")
def add_context_embedding(texts, contexts):
    new_texts = []
    for i, text in enumerate(texts):
        context = contexts.get(i)  # return None if the key is not found
        if context is not None:
            new_text = context['before'] + text + context['after']
            new_texts.append(new_text)
        else:
            new_texts.append(text)  # handle the case where context is not found
    return new_texts


def main():
    global current_val_loss
    with open('train.json', 'r', encoding='utf-8') as file:
        data = json.load(file)

    texts = [item['text'] for item in data]
    labels = [item['sentiment'] for item in data]
    label_dict = {'positive': 0, 'neutral': 1, 'negative': 2}
    labels = [label_dict[label] for label in labels]


    kf = KFold(n_splits=4) # 25/75 - change.. 25/75......
    fold_results = []
    #print("split dataset 75/25\n")
    for fold, (train_index, val_index) in enumerate(kf.split(data)):
        train_data = [data[i] for i in train_index]
        val_data = [data[i] for i in val_index]
        model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=3)
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model.to(device)
        train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.25, random_state=42)

        # Calculate class weights
        unique_classes = np.unique(train_labels)
        weights = class_weight.compute_class_weight(class_weight='balanced', classes=unique_classes, y=train_labels)
        class_weights = dict(zip(unique_classes, weights))
        #print(class_weights)
        #class_weights = compute_class_weight('balanced', np.unique(train_labels), train_labels)
        class_weights_list = [class_weights[key] for key in sorted(class_weights.keys())]
        weights_tensor = torch.tensor(class_weights_list, dtype=torch.float)

        # apply resampling techniques to the training data
        vectorizer = TfidfVectorizer()
        X_train_vect = vectorizer.fit_transform(train_texts)
        sm = SMOTE(random_state=42)
        X_res, y_res = sm.fit_resample(X_train_vect, train_labels)
        #print(f'X_res{X_res}')
        #print(f'y_res{y_res}')


        # transforming my text
        cleaned_train_texts = [clean_text(text) for text in train_texts]
        cleaned_val_texts = [clean_text(text) for text in val_texts]

        removed_stop_words_train_texts = remove_stop_words(cleaned_train_texts)
        removed_stop_words_val_texts = remove_stop_words(cleaned_val_texts)

        for _ in train_texts:
            additional_context_train = {'before': 'Previous sentence.', 'after': 'Next sentence.'}
        for _ in val_texts:
            additional_context_val = {'before': 'Previous sentence.', 'after': 'Next sentence.'}
        train_texts_with_context = add_context_embedding(cleaned_train_texts, additional_context_train)
        val_texts_with_context = add_context_embedding(cleaned_val_texts, additional_context_val)

        #print("tokenization\n")
        tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')  # DistilBertTokenizer instead of BertTokenizer, because it's smaller and faster working
        train_encodings = tokenizer.batch_encode_plus(train_texts_with_context, max_length=32, padding='longest', truncation=True)  # 512->256 for time optimizing
        val_encodings = tokenizer.batch_encode_plus(val_texts_with_context, max_length=32, padding='longest', truncation=True)  # 512->256 for time optimizing


        loss_fun = torch.nn.CrossEntropyLoss(weight=weights_tensor)
        print(f"loss function in the beginning: {loss_fun}")

        train_dataset = SentimentDataset(train_encodings, train_labels)
        val_dataset = SentimentDataset(val_encodings, val_labels)

        #print("BERT-model\n")
        train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, pin_memory=True) # batch size changed 8->16 for generalization of the model
        val_loader = DataLoader(val_dataset, batch_size=4)
        optimizer = AdamW(model.parameters(), lr=9e-5, eps=1e-6) # l2-regularization: changed for speed up the training 1. learning rate, 2. exclude division by zero, 3. regularization via penalty
        scaler = GradScaler()  # for mixed training


        print("training\n")
        new_learning_rate = None
        best_val_loss = 10.0 # it'll stop, when the validation loss will start increasing
        early_stopping_counter = 0
        num_epochs = 5
        best_accuracy = 0.0
        log_loss_value = 0.0
        bal_acc = 0.0
        f1 = 0.0
        accuracy = 0.0
        predicted_probabilities = None
        for epoch in range(num_epochs):
            model.train()
            #print(f"Total batches: {len(train_loader)}")
            for batch in train_loader:
                #print(f"Processing batch {batch_index}")
                #print("Input IDs shape:", batch['input_ids'].shape)  # Should be (batch_size, sequence_length)
                #print("Attention Mask shape:", batch['attention_mask'].shape)  # Should be (batch_size, sequence_length)
                optimizer.zero_grad()
                inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}
                labels = batch['labels'].to(device)
                outputs = model(**inputs, labels=labels)
                loss = outputs.loss
                loss.backward()
                optimizer.step()
                print("loss:", loss)

            print("mixed precision training\n") # it allows us to take 16-bit floating point, 32-bit floating point & speed us up
            model.eval()
            predictions = []
            true_labels = []
            with autocast():
                outputs = model(**inputs, labels=labels)
                loss = outputs.loss
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()


            print("validation loop\n")
            model.eval()
            predictions = []
            true_labels = []
            with torch.no_grad():
                for batch in val_loader:
                    #inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}
                    #labels = batch['labels'].to(device)
                    #outputs = model(**inputs)
                    #preds = torch.argmax(outputs.logits, dim=1)
                    #predictions.extend(preds.cpu().numpy())
                    #true_labels.extend(labels.cpu().numpy())
                    #logits = outputs.logits
                    #probs = softmax(logits, dim=1)
                    #probabilities.extend(probs.cpu().numpy())
                    inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}
                    labels = batch['labels'].to(device)
                    outputs = model(**inputs)
                    #loss = loss_fun(outputs.logits.to(device), batch['labels'].to(device))
                    loss = outputs.loss
                    preds = torch.argmax(outputs.logits, dim=1)
                    predictions.extend(preds.cpu().numpy())
                    true_labels.extend(labels.cpu().numpy())
                    print("val_loss", loss)

            print("confusion_matrix\n")
            cm = confusion_matrix(true_labels, predictions,
                                  labels=[0, 1, 2])  # labels should be the sorted unique classes
            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            print(f"cm output:\n {cm}")
            plt.figure(figsize=(10, 7))
            #sns.heatmap(cm_normalized, annot=True, fmt=".2f", cmap='Blues',
            #            xticklabels=['Label1', 'Label2', 'Label3'],
            #            yticklabels=['Label1', 'Label2', 'Label3'])
            plt.title('Normalized Confusion Matrix')
            plt.xlabel('Predicted')
            plt.ylabel('True')
            plt.show()

            #if cm.size > 0 and not np.all(cm == cm[0, 0]):
            #    fig, ax = plt.subplots()
            #    cax = ax.matshow(cm)
            #    plt.title('Confusion Matrix')
            #    plt.xlabel('Predicted')
            #    plt.ylabel('True')
            #    plt.colorbar(cax)
            #    try:
            #       sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            #                  xticklabels=['Label1', 'Label2'],
            #                  yticklabels=['Label1', 'Label2'])
            #        print("confusion matrix is complete!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
            #   except Exception as e:
            #        print(f"advanced info into cm is incorrect with error: {e}")
            #    plt.show()
            #else:
            #    print("Invalid confusion matrix data, namely, if cm.size > 0 and not np.all(cm == cm[0, 0]): !!!!!")

            #incorrect_predictions = np.where(np.array(true_labels) != np.array(predictions))[0]
            #for index in incorrect_predictions[5:]:  # check last 5 incorrect predictions
                #print(f"Text: {val_texts[index]}") - sentences
                #print(f"True Label: {true_labels[index]}")
                #print(f"Predicted Label: {predictions[index]}\n")
            #correct_predictions = np.where(np.array(true_labels) == np.array(predictions))[0]
            #for index in correct_predictions[5:]:  # check first 5 incorrect predictions
                #print(f"Text: {val_texts[index]}") # sentences
                #print(f"True Label: {true_labels[index]}")
                #print(f"Predicted Label: {predictions[index]}\n")


            #if len(train_loader) != 0:
            #    total_loss = total_loss/len(train_loader)
            #    print(f"Epoch {epoch} is finished, Loss(total_loss/len(train_loader)): {total_loss/len(train_loader)}")
            #else:
            #    print(f"Epoch {epoch + 1} finished, Loss: Undefined DIVISION BY  ZERO!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")

            # check for improvement
            if current_val_loss < best_val_loss:
                best_val_loss = current_val_loss
                early_stopping_counter = 0
                #torch.save(model.state_dict(), 'best_model_state.bin') # temporary save
            else: # occur, if we stopped development
                early_stopping_counter += 1
                print("current_val_loss > best_val_loss\n")
                print(f"Epoch(epoch as output): {epoch} of {num_epochs} epoches ; Training Loss(average training loss as output): "
                      f"{loss:.3f} - loss, Validation Loss(current val loss as output): {current_val_loss:.3f}")
                if early_stopping_counter > 3:
                    for param_group in optimizer.param_groups:
                        new_learning_rate = max(param_group['lr'] - 1e-5, 1e-6)  # delta will improve the model to the minimal
                        param_group['lr'] = new_learning_rate
                    early_stopping_counter = 0  # reset counter
                    print(f"No improvement for {epoch} consecutive epochs => decrease learning rate")
                    if new_learning_rate == 1e-6:
                        break

            if epoch == num_epochs:
                folder_name = datetime.now().strftime('%Y-%m-%d_%H-%M-%S') # generating new folder
                model_path = f'saved_models/{folder_name}/model'
                tokenizer_path = f'saved_models/{folder_name}/tokenizer'
                print("model&tokenizer saved\n")
                model.save_pretrained(f'C:/Users/user/pythonProject/{model_path}') # save the model
                tokenizer.save_pretrained(f'C:/Users/user/pythonProject/{tokenizer_path}') # save the tokenizer

        # if we saved temporary results, we can open them out here
        #best_model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased')
        #best_model.load_state_dict(torch.load('best_model_state.bin'))

        # evaluation on best model
        print("evaluation\n")
        #model.load_state_dict(torch.load('best_model.pth'))
        model.eval()
        predictions = []
        true_labels = []
        with torch.no_grad():
            for batch in val_loader:
                inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}
                labels = batch['labels'].to(device)
                outputs = model(**inputs)
                preds = torch.argmax(outputs.logits, dim=1)
                predictions.extend(preds.cpu().numpy())
                true_labels.extend(labels.cpu().numpy())
                predicted_probabilities = softmax(outputs.logits, dim=1)

            print("current_metrics\n")
            '''
            if len(true_labels) != len(predictions):
                print("Error handling in accuracy: len(true_labels) != len(predictions)!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n")
                print(f"true_labels:{true_labels} ; predictions: {predictions} ; len(predictions): {len(predictions)}")
            else:
                accuracy = accuracy_score(true_labels, predictions)
            if accuracy > best_accuracy:
                print("improvement of accuracy")
                best_accuracy = accuracy
                torch.save(model.state_dict(), 'model_state_dict.pt') # it'd be the most important metric (by default)

            if len(true_labels) != len(predictions):
                print("Error handling in f1: len(true_labels) != len(predictions)!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
            else:
                f1 = f1_score(true_labels, predictions)  

            if len(true_labels) != len(predictions):
                print("Error handling in bal_acc: len(true_labels) != len(predictions)!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
            else:
                bal_acc = balanced_accuracy_score(true_labels, predictions)
            if 0 < len(predictions) == len(true_labels):
                predicted_probabilities = model.predict_proba(data)
                log_loss_value = log_loss(true_labels, predicted_probabilities, labels=[0, 1, 2])
            else:
                log_loss_value = 1000000 # take it as max
                print(f"some problem arose with 0 < len(predictions) == len(true_labels) ; "
                      f"len(predictions):{len(predictions)} ; len(true_labels):{len(true_labels)} ; "
                      f"predictions:{predictions} ; true_labels:{true_labels} !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
            '''

            accuracy = accuracy_score(true_labels, predictions)
            f1 = f1_score(true_labels, predictions)
            bal_acc = balanced_accuracy_score(true_labels, predictions)
            predicted_probabilities = model.predict_proba(data)
            log_loss_value = log_loss(true_labels, predicted_probabilities, labels=[0, 1, 2])

            print("into the cycle")
            print(f"Accuracy: {accuracy}")
            print(f"F1 Score: {f1}")
            print(f"balanced_accuracy_score: {bal_acc}")
            print(f"log_loss : {log_loss_value}")
        '''
        # metrics
        print("metrics:")
        # macro - compute performance of every metric separately - find the disbalanced weights of every metrics
        # weighted - compute performance of every metric - find the mean value of weights by suppors
        accuracy = accuracy_score(true_labels, predictions)
        f1 = f1_score(true_labels, predictions, average='macro')
        log_loss_value = log_loss(true_labels, predictions, average='macro')
        bal_acc = balanced_accuracy_score(true_labels, predictions)

        #showing the results
        print(f"the final results of {epoch} epoch:")
        print(f"Accuracy: {accuracy}")
        print(f"F1 Score: {f1}")
        if log_loss_value != None:
            print(f"log_loss : {log_loss_value}")
        print(f"balanced_accuracy_score: {bal_acc}")
        '''

        print("out of cycle")
        print(f"Accuracy: {accuracy}")
        print(f"F1 Score: {f1}")
        print(f"balanced_accuracy_score: {bal_acc}")
        print(f"log_loss : {log_loss_value}")

        #saving the results
        fold_results.append(accuracy)
        fold_results.append(f1)
        if log_loss_value != None:
            fold_results.append(log_loss_value)
        fold_results.append(bal_acc)

if __name__ == '__main__':
    main()
