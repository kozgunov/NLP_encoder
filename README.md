# NLP_encoder
creating the large language model with Abramov Maxim and Valerii Olisenko. 10.2023-05.2024

1. verison: mvp-model, which is just working, no more
2. version: strictly improved the time complexity & model became more flexible, visualization
3. version: wrote the function seq2seq, exception handling, more metrics, c_m, saving the models
3.1 version: handled the specific exceptions, right/wrong examples showed
4. version: added more perfomance metrics for multiclass classification, balanced the classes, early stopping, dropout, hadling over/under-fitting, reconfig of BERT, fixed remarks(from 3rd),purification, context


# NLP Sequence Classification with BERT

## Project Description
This project applies a BERT model to a sequence classification task in NLP. It includes the full pipeline from data preprocessing, model training, to performance evaluation.

## Installation
To set up this project:
1. Clone the repo with the latest version: https://github.com/kozgunov/NLP_encoder/blob/main/5th%20version
2. Install required packages locally
3. write your text and get the sentimental replies

## Usage
Run the main script to train and evaluate the model:


## Features
- BERT-based sequence classification
- Customizable model parameters
- Comprehensive performance metrics evaluation

## Contributing
Contributions to this project are welcome. Please submit pull requests or open an issue for changes you would like to make.

## License
This project is licensed under the MIT License - see the LICENSE file for details.

## Contact
For any queries, please open an issue on the GitHub repository.

## Acknowledgments
- Transformers library by Hugging Face
- PyTorch community


